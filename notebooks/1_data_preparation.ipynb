{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8026498,"sourceType":"datasetVersion","datasetId":4730377}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1.Importing the dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport glob\nimport json\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2025-09-17T13:54:29.545544Z","iopub.execute_input":"2025-09-17T13:54:29.545783Z","iopub.status.idle":"2025-09-17T13:54:31.927286Z","shell.execute_reply.started":"2025-09-17T13:54:29.545752Z","shell.execute_reply":"2025-09-17T13:54:31.926611Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading Model and tokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T13:42:18.048049Z","iopub.status.idle":"2025-09-17T13:42:18.048432Z","shell.execute_reply.started":"2025-09-17T13:42:18.048234Z","shell.execute_reply":"2025-09-17T13:42:18.048247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sbert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = \"IN-Abs\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-09-17T13:42:26.761159Z","iopub.execute_input":"2025-09-17T13:42:26.761593Z","iopub.status.idle":"2025-09-17T13:42:26.767212Z","shell.execute_reply.started":"2025-09-17T13:42:26.761559Z","shell.execute_reply":"2025-09-17T13:42:26.766004Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_root_path():\n    #function to get root path of dataset\n    path = \"/kaggle/input/dataset/\"\n    return path","metadata":{"execution":{"iopub.status.busy":"2025-09-17T13:42:30.261120Z","iopub.execute_input":"2025-09-17T13:42:30.261511Z","iopub.status.idle":"2025-09-17T13:42:30.266537Z","shell.execute_reply.started":"2025-09-17T13:42:30.261481Z","shell.execute_reply":"2025-09-17T13:42:30.265378Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_summary_data(dataset,folder):\n    #function to get names, documents, and summaries\n    path = get_root_path() + dataset + '/' + folder + '/judgement'\n    all_files = glob.glob(path + \"/*.txt\")\n    data_source = []\n    names = []\n    for filename in all_files:\n        with open(filename, 'r') as f:\n            p = filename.rfind(\"/\")\n            names.append(filename[p+1:])\n            a = f.read()\n            data_source.append(a)\n    path = get_root_path() + dataset + '/' + folder + '/summary'\n    all_files = glob.glob(path + \"/*.txt\")\n    data_summary = []\n    for filename in all_files:\n        with open(filename, 'r') as f:\n            a = f.read()\n            l = len(a)\n            data_summary.append(a)\n\n    return names, data_source, data_summary","metadata":{"execution":{"execution_failed":"2025-09-17T13:52:44.798Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Reading the documents and summaries \nnames, data_source, data_summary = get_summary_data(dataset, \"train-data\")\nprint(len(names))\nprint(len(data_source))\nprint(len(data_summary))","metadata":{"execution":{"execution_failed":"2025-09-17T13:52:44.798Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_to_sentences(para):\n    sents = nltk.sent_tokenize(para)\n    return sents","metadata":{"execution":{"iopub.status.busy":"2024-04-28T05:47:50.225262Z","iopub.execute_input":"2024-04-28T05:47:50.225620Z","iopub.status.idle":"2024-04-28T05:47:50.230220Z","shell.execute_reply.started":"2024-04-28T05:47:50.225594Z","shell.execute_reply":"2024-04-28T05:47:50.229282Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def similarity_l_l(l1, l2):\n    '''\n    Function to find the most similar sentence in the document for each sentence in the summary \n    input:  l1 - Summary sentences\n            l2 - Document sentences\n    returns a list of document sentence indexes for each sentence in the summary \n    '''\n    document_embeddings = sbert_model.encode(l1+l2)\n    similarities=cosine_similarity(document_embeddings)\n    \n    result = []\n    for i in range(len(l1)):\n        vals = similarities[i]\n        vals = vals[len(l1):]\n        idx = np.argmax(vals)\n        result.append(idx)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-04-28T05:49:13.502529Z","iopub.execute_input":"2024-04-28T05:49:13.503524Z","iopub.status.idle":"2024-04-28T05:49:13.509862Z","shell.execute_reply.started":"2024-04-28T05:49:13.503486Z","shell.execute_reply":"2024-04-28T05:49:13.508886Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def nest_sentencesV2(document,chunk_length):\n    '''\n    function to chunk a document\n    input:  document           - Input document\n            chunk_length        - chunk length\n    output: list of chunks. Each chunk is a list of sentences.\n    '''\n    nested = []\n    sent = []\n    length = 0\n    for sentence in nltk.sent_tokenize(document):\n        length += len(sentence.split(\" \"))\n        if length < chunk_length:\n            sent.append(sentence)\n        else:\n            nested.append(sent)\n            sent = []\n            sent.append(sentence)\n            length = 0\n    if len(sent)>0:\n        nested.append(sent)\n    return nested","metadata":{"execution":{"iopub.status.busy":"2024-04-28T05:49:16.000692Z","iopub.execute_input":"2024-04-28T05:49:16.001393Z","iopub.status.idle":"2024-04-28T05:49:16.008035Z","shell.execute_reply.started":"2024-04-28T05:49:16.001357Z","shell.execute_reply":"2024-04-28T05:49:16.006969Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_chunks_data_from_docV2(doc, summ):\n    '''\n    Function to generate chunks along with their summaries \n    input:  doc - legal Document\n            summ - Gold standard summary\n    returns a list of chunks and their summaries \n    '''\n    chunk_summ_word_threshold = 150\n    sentence_mapping = {}\n    doc_sents = split_to_sentences(doc)\n    summ_sents = split_to_sentences(summ)\n    \n    result = (similarity_l_l(summ_sents,doc_sents))\n    \n    for i in range(len(summ_sents)):\n        sentence_mapping[doc_sents[result[i]]] = summ_sents[i]\n    \n    final_chunks = []\n    final_summ = []\n    for chunk in nest_sentencesV2(doc, 1024):\n        summ = \"\"\n        for chunk_sent in chunk:\n            if chunk_sent in sentence_mapping:\n                summ = summ + sentence_mapping[chunk_sent]\n        if len(summ.split(\" \")) >= chunk_summ_word_threshold:\n            final_chunks.append(\" \".join(chunk))\n            final_summ.append(summ)\n    return final_chunks, final_summ\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T05:49:17.184411Z","iopub.execute_input":"2024-04-28T05:49:17.184761Z","iopub.status.idle":"2024-04-28T05:49:17.192327Z","shell.execute_reply.started":"2024-04-28T05:49:17.184735Z","shell.execute_reply":"2024-04-28T05:49:17.191297Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loop to pass every document, generate the fine tuning data and saving in a excel file \nimport pandas as pd\ntraining_chunks = []\ntraining_summs = []\nfor i in tqdm(range(len(data_source))):\n    cks, summs = get_chunks_data_from_docV2(data_source[i],data_summary[i])\n    training_chunks = training_chunks + cks\n    training_summs = training_summs + summs\n#     print(i, len(training_summs), end = \", \", sep = \" : \")\n    if i%100 == 0: \n        full = list(zip(training_chunks,training_summs))\n        df = pd.DataFrame(full,columns=['data', 'summary'])\n        df.to_excel(\"FD_\" + dataset + \"_CSM_BK_512.xlsx\")\n#         break\nfull = list(zip(training_chunks,training_summs))\ndf = pd.DataFrame(full,columns=['data', 'summary'])\ndf.to_excel(\"FD_\" + dataset + \"_CSM_512.xlsx\")","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}