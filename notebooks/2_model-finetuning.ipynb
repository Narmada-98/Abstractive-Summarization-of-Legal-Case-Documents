{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8251602,"sourceType":"datasetVersion","datasetId":4896186}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1.Importing the dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport csv\nimport math\nimport random\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:27:43.640178Z","iopub.execute_input":"2025-09-17T16:27:43.640374Z","iopub.status.idle":"2025-09-17T16:27:51.907825Z","shell.execute_reply.started":"2025-09-17T16:27:43.640357Z","shell.execute_reply":"2025-09-17T16:27:51.907150Z"}},"outputs":[{"name":"stderr","text":"2025-09-17 16:27:49.273146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758126469.295833     167 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758126469.302636     167 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2.Load Model and Tokenizer ","metadata":{}},{"cell_type":"code","source":"model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large', add_prefix_space=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:28:37.994682Z","iopub.execute_input":"2025-09-17T16:28:37.994982Z","iopub.status.idle":"2025-09-17T16:28:40.507150Z","shell.execute_reply.started":"2025-09-17T16:28:37.994961Z","shell.execute_reply":"2025-09-17T16:28:40.506537Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## 3.Configuration","metadata":{}},{"cell_type":"code","source":"# --- Multi-GPU setup ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = torch.nn.DataParallel(model)  # wrap your model\nmodel = model.to(DEVICE)\n\n# --- CSV setup ---\nlog_file = \"training_log.csv\"\nheader = [\"epoch\", \"train_loss\", \"val_loss\"]\nif not os.path.exists(log_file):\n    with open(log_file, mode='w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(header)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:30:10.830208Z","iopub.execute_input":"2025-09-17T16:30:10.831019Z","iopub.status.idle":"2025-09-17T16:30:11.368196Z","shell.execute_reply.started":"2025-09-17T16:30:10.830988Z","shell.execute_reply":"2025-09-17T16:30:11.367622Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"DATA_FILE = r\"/kaggle/input/FD_IN-Abs_CSM_512.xlsx\"   # your Excel file with 'source' and 'target' columns\nBATCH_SIZE = 4\nEPOCHS = 3\nLR = 5e-5\nMAX_INPUT = 1024\nMAX_TARGET = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:31:01.996140Z","iopub.execute_input":"2025-09-17T16:31:01.997009Z","iopub.status.idle":"2025-09-17T16:31:02.001304Z","shell.execute_reply.started":"2025-09-17T16:31:01.996973Z","shell.execute_reply":"2025-09-17T16:31:02.000540Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 3.Dataset","metadata":{}},{"cell_type":"code","source":"class SummaryDataset(Dataset):\n    def __init__(self, tokenizer, sources, targets):\n        self.tokenizer = tokenizer\n        self.sources = sources\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.sources)\n\n    def __getitem__(self, idx):\n        src = self.sources[idx]\n        tgt = self.targets[idx]\n        src_enc = self.tokenizer(src, max_length=MAX_INPUT, padding='max_length',\n                                 truncation=True, return_tensors='pt')\n        tgt_enc = self.tokenizer(tgt, max_length=MAX_TARGET, padding='max_length',\n                                 truncation=True, return_tensors='pt')\n        labels = tgt_enc['input_ids'].squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100  # ignore pad in loss\n        return {\n            'input_ids': src_enc['input_ids'].squeeze(),\n            'attention_mask': src_enc['attention_mask'].squeeze(),\n            'labels': labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:31:04.960122Z","iopub.execute_input":"2025-09-17T16:31:04.960962Z","iopub.status.idle":"2025-09-17T16:31:04.968711Z","shell.execute_reply.started":"2025-09-17T16:31:04.960923Z","shell.execute_reply":"2025-09-17T16:31:04.967902Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.read_excel(DATA_FILE)\ndf = df.rename(columns={'data': 'source', 'summary': 'target'}) if 'data' in df.columns else df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:31:05.775919Z","iopub.execute_input":"2025-09-17T16:31:05.776235Z","iopub.status.idle":"2025-09-17T16:31:07.796475Z","shell.execute_reply.started":"2025-09-17T16:31:05.776210Z","shell.execute_reply":"2025-09-17T16:31:07.795413Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dataset = SummaryDataset(tokenizer, df['source'].tolist(), df['target'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:31:09.879983Z","iopub.execute_input":"2025-09-17T16:31:09.880929Z","iopub.status.idle":"2025-09-17T16:31:09.887957Z","shell.execute_reply.started":"2025-09-17T16:31:09.880893Z","shell.execute_reply":"2025-09-17T16:31:09.887117Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_len = int(0.8 * len(dataset))\nval_len = len(dataset) - train_len\ntrain_ds, val_ds = random_split(dataset, [train_len, val_len])\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:31:10.746965Z","iopub.execute_input":"2025-09-17T16:31:10.747557Z","iopub.status.idle":"2025-09-17T16:31:10.752735Z","shell.execute_reply.started":"2025-09-17T16:31:10.747529Z","shell.execute_reply":"2025-09-17T16:31:10.751991Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 4.Training Loop","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=LR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T16:31:35.469738Z","iopub.execute_input":"2025-09-17T16:31:35.470324Z","iopub.status.idle":"2025-09-17T16:31:35.475684Z","shell.execute_reply.started":"2025-09-17T16:31:35.470297Z","shell.execute_reply":"2025-09-17T16:31:35.474683Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# --- Training loop ---\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        outputs = model(input_ids=batch['input_ids'].to(DEVICE),\n                attention_mask=batch['attention_mask'].to(DEVICE),\n                labels=batch['labels'].to(DEVICE))\n        loss = outputs.loss\n        if loss.dim() > 0:\n            loss = loss.mean()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch}: train loss {avg_train_loss:.4f}\")\n\n    # --- Validation ---\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_loader:\n            outputs = model(input_ids=batch['input_ids'].to(DEVICE),\n                            attention_mask=batch['attention_mask'].to(DEVICE),\n                            labels=batch['labels'].to(DEVICE))\n            val_loss += outputs.loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Epoch {epoch}: val loss {avg_val_loss:.4f}\")\n\n    # --- Save metrics to CSV ---\n    with open(log_file, mode='a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([epoch, avg_train_loss, avg_val_loss])\n\n    # --- Save model checkpoint ---\n    checkpoint_path = f\"model_epoch_{epoch}.pt\"\n    if isinstance(model, torch.nn.DataParallel):\n        torch.save(model.module.state_dict(), checkpoint_path)  # unwrap for DataParallel\n    else:\n        torch.save(model.state_dict(), checkpoint_path)\n\n    print(f\"Saved model checkpoint: {checkpoint_path}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}